{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurvaChiniya/Aspect-based-sentiment-analysis/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g1BZqbDsTRXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ea6f09-ca38-4b1e-a67b-a207594c4ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uAgatyqTTT4O"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "from transformers import logging\n",
        "logging.set_verbosity_warning()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# declare parameters\n",
        "max_length = 160\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "lr  = 0.01\n",
        "locations = ['LOCATION1', 'LOCATION2']\n",
        "aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety','shopping', 'touristy', 'transit-location']\n",
        "label_to_int = {\n",
        "    'Positive': 0,\n",
        "    'Negative': 1,\n",
        "    'None': 2\n",
        "}\n",
        "# load data\n",
        "train_data = pd.read_csv('/content/training_set.csv')[:1000]\n",
        "val_data = pd.read_csv('/content/validation_set.csv')[:600]\n",
        "#convert text sentiment to int labels\n",
        "train_data[\"sentiment\"]=train_data.apply(lambda row:label_to_int[row.sentiment],axis=1)\n",
        "val_data[\"sentiment\"]=val_data.apply(lambda row:label_to_int[row.sentiment],axis=1)\n",
        "# drop values\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lowercase=True)\n",
        "labels = train_data['sentiment'].values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T4bIIPtNYSQ",
        "outputId": "2c9af8bb-b75b-44f9-9a62-d8a788c393b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id  ... sentiment\n",
            "0    1430  ...         2\n",
            "1    1430  ...         2\n",
            "2    1430  ...         2\n",
            "3    1430  ...         2\n",
            "4    1430  ...         2\n",
            "..    ...  ...       ...\n",
            "995   760  ...         2\n",
            "996  1605  ...         2\n",
            "997  1605  ...         0\n",
            "998  1605  ...         2\n",
            "999  1605  ...         2\n",
            "\n",
            "[1000 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentiHood:\n",
        "  \"\"\"\n",
        "  This class tokenizes the input text using the pre-trained BERT tokenizer \n",
        "  (wordpiece) and returns the corresponding tensors.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, text, auxiliary_sentence, targets, tokenizer, max_len):\n",
        "    self.text = text\n",
        "    self.auxiliary_sentence = auxiliary_sentence\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.targets = targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "    auxiliary_sentence = str(self.auxiliary_sentence[item])\n",
        "    targets = self.targets[item]\n",
        "    text = text + ' ' + auxiliary_sentence\n",
        "\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens = True,\n",
        "        max_length = self.max_len,\n",
        "        pad_to_max_length = True\n",
        "    )\n",
        "\n",
        "    ids = inputs[\"input_ids\"]\n",
        "    mask = inputs[\"attention_mask\"]\n",
        "    token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "    return {\"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "        \"targets\": torch.tensor(targets, dtype=torch.long),\n",
        "        \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "        \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "        \n",
        "    }\n"
      ],
      "metadata": {
        "id": "g07LM8V1zjvM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training dataloader\n",
        "train_dataset = SentiHood(\n",
        "    text = train_data['text'].values,\n",
        "    auxiliary_sentence = train_data['auxiliary_sentence'],\n",
        "    targets = train_data['sentiment'].values,\n",
        "    tokenizer = tokenizer,\n",
        "    max_len = max_length\n",
        ")\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False\n",
        ")\n",
        "print(len(train_dataset))\n",
        "print(len(train_data_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6gn2N2OyXio",
        "outputId": "a61b2991-1628-4791-c2e5-9d935ba8a406"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define val dataloader\n",
        "val_dataset = SentiHood(\n",
        "    text = val_data['text'].values,\n",
        "    auxiliary_sentence = train_data['auxiliary_sentence'],\n",
        "    targets = val_data['sentiment'].values,\n",
        "    tokenizer = tokenizer,\n",
        "    max_len = max_length\n",
        ")\n",
        "val_data_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False\n",
        ")\n",
        "print(len(val_dataset))\n",
        "print(len(val_data_loader))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i3o1MAggq0-",
        "outputId": "62888728-6724-4fba-df82-8f083cf80937"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "19\n",
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self, bert_model):\n",
        "    super(Model, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(bert_model, return_dict=False)\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, 3) # Number of output classes = 3\n",
        "\n",
        "  def forward(self, ids, mask, token_type_ids):\n",
        "    last_hidden_state, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)\n",
        "model = Model(\"bert-base-uncased\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5IYFAaHixXY",
        "outputId": "30f55c2c-8f97-4fd3-a093-811a7ed9e7b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to initalize weights in proportion to imbalance in classes\n",
        "class_counts = []\n",
        "for i in range(3):\n",
        "  class_counts.append(train_data[train_data['sentiment']==i].shape[0])\n",
        "print(f\"Class Counts: {class_counts}\")\n",
        "num_train_steps = int(len(train_dataset) / batch_size * epochs)\n",
        "optimizer=optim.Adagrad(model.parameters(),lr=lr)\n",
        "scheduler =StepLR(\n",
        "    optimizer,\n",
        "    gamma=0.8,\n",
        "    step_size = 1\n",
        ")\n",
        "print(num_train_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9pOYCfUO8uB",
        "outputId": "fa2aa0bf-33d7-45c8-8847-e8b95626d7e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Counts: [74, 30, 896]\n",
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xeIzrcW4g1A",
        "outputId": "95d9581c-8cd9-4296-cc56-3da78bc8319d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_function(data_loader, model, optimizer, device):\n",
        "  \"\"\"\n",
        "  This function defines the training loop over the entire training set.\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for bi, d in enumerate(data_loader):\n",
        "    ids = d[\"ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    token_type_ids = d[\"token_type_ids\"]\n",
        "    targets = d[\"targets\"]\n",
        "\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    targets = targets.to(device, dtype=torch.long)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "    summed = sum(class_counts)\n",
        "    weight = torch.tensor(class_counts) / summed\n",
        "    loss = nn.CrossEntropyLoss(weight=weight)(outputs, targets)\n",
        "\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if bi % 10 == 0 and bi!=0:\n",
        "      temp = f'Batch index = {bi}\\tLoss = {running_loss/10}'\n",
        "      print(temp)\n",
        "\n",
        "      f1 = open('/content/loss.txt', 'a+')\n",
        "      temp = temp + '\\n'\n",
        "      f1.write(temp)\n",
        "      f1.close()\n",
        "\n",
        "      running_loss = 0.0"
      ],
      "metadata": {
        "id": "KDqnyhhf2yuO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_function(data_loader, model, device):\n",
        "  \"\"\"\n",
        "  This function defines the evaluation loop over the entire validation set.\n",
        "  It also computes accuracy of the trained model, which is used to select the \n",
        "  best model.\n",
        "  \"\"\"\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  corrects = 0\n",
        "  total = 0\n",
        "  for bi, d in enumerate(data_loader):\n",
        "    ids = d[\"ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    token_type_ids = d[\"token_type_ids\"]\n",
        "    targets = d[\"targets\"]\n",
        "\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    targets = targets.to(device, dtype=torch.long)\n",
        "\n",
        "    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = total + targets.size(0)\n",
        "    corrects = corrects + (predicted==targets).sum().item()\n",
        "\n",
        "    print(f\"bi: {bi}\\tPredicted: {predicted}\\tTargets: {targets}\")\n",
        "\n",
        "  accuracy = corrects / total * 100\n",
        "  f1 = open('/content/accuracy.txt', 'a+')\n",
        "  temp = f\"Corrects: {corrects}\\tTotal: {total}\\tAccuracy: {accuracy}\\n\"\n",
        "  f1.write(temp)\n",
        "  f1.close()\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "ylrSPnBS3NAM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ckD0L52kQdVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BhiTJhl3T8a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dd99e3-c13a-49ba-de44-95e0a4016d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch index = 10\tLoss = 1.2250111788511275\n",
            "Batch index = 20\tLoss = 0.13874215632677078\n",
            "Batch index = 30\tLoss = 0.11299104765057563\n",
            "bi: 0\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 1\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 2\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 3\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 4\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 5\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 6\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
            "        2, 0, 2, 0, 2, 2, 2, 2])\n",
            "bi: 7\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([1, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 0, 2, 2, 2])\n",
            "bi: 8\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 0, 2, 2, 2, 2, 2])\n",
            "bi: 9\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 0, 2, 2])\n",
            "bi: 10\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 0])\n",
            "bi: 11\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 12\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
            "        2, 0, 2, 2, 2, 2, 2, 2])\n",
            "bi: 13\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        1, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 14\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 1, 2, 2, 2, 2, 2])\n",
            "bi: 15\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 16\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
            "        2, 2, 2, 2, 2, 1, 2, 2])\n",
            "bi: 17\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 18\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "\n",
            "Epoch = 0\tAccuracy Score = 94.16666666666667\n",
            "Learning Rate = 0.01\n",
            "\n",
            "Batch index = 10\tLoss = 0.09767806753516198\n",
            "Batch index = 20\tLoss = 0.0971747487783432\n",
            "Batch index = 30\tLoss = 0.08237757571041585\n",
            "bi: 0\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 1\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 2\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 3\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 4\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 5\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 6\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
            "        2, 0, 2, 0, 2, 2, 2, 2])\n",
            "bi: 7\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([1, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 0, 2, 2, 2])\n",
            "bi: 8\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 0, 2, 2, 2, 2, 2])\n",
            "bi: 9\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 0, 2, 2])\n",
            "bi: 10\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 0])\n",
            "bi: 11\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 12\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2,\n",
            "        2, 0, 2, 2, 2, 2, 2, 2])\n",
            "bi: 13\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        1, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 14\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 1, 2, 2, 2, 2, 2])\n",
            "bi: 15\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 16\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
            "        2, 2, 2, 2, 2, 1, 2, 2])\n",
            "bi: 17\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "bi: 18\tPredicted: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\tTargets: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "\n",
            "Epoch = 1\tAccuracy Score = 94.16666666666667\n",
            "Learning Rate = 0.0064\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "  train_function(data_loader=train_data_loader, model=model, optimizer=optimizer, device=device)\n",
        "  accuracy = eval_function(data_loader=val_data_loader, model=model, device=device)\n",
        "\n",
        "  print(f\"\\nEpoch = {epoch}\\tAccuracy Score = {accuracy}\")\n",
        "  print(f\"Learning Rate = {scheduler.get_lr()[0]}\\n\")\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  torch.save(model, '/content/' + str(epoch) + '.bin')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aIA8xhE2qX_e"
      },
      "execution_count": 76,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "train",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLKHKYip0qS+O9cYgwnXNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}