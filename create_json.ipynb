{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_json",
      "provenance": [],
      "authorship_tag": "ABX9TyMLc9gTBHJ5KT0Wzw9RjF9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurvaChiniya/Aspect-based-sentiment-analysis/blob/main/create_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4oluy-hk49H",
        "outputId": "6361577b-9ccb-4806-8270-db519aa53944"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install transformers\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjgsVc46lem2",
        "outputId": "4ff94778-e435-46b5-fd78-ef3dac032963"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.7.0\n",
            "Uninstalling tensorflow-2.7.0:\n",
            "  Successfully uninstalled tensorflow-2.7.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 503 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zU6r7LdkiHP",
        "outputId": "60323cb5-1e94-420a-feba-48c5afeb5f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# declare parameters\n",
        "max_length = 160\n",
        "batch_size = 32\n",
        "locations = ['LOCATION1', 'LOCATION2']\n",
        "aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety','shopping', 'touristy', 'transit-location']\n",
        "labels_to_sentiment = {\n",
        "    0: 'Positive',\n",
        "    1: 'Negative',\n",
        "    2: 'None'\n",
        "}\n",
        "bert_model = 'bert-base-uncased'\n",
        "with open('/content/drive/MyDrive/sentihood/sentihood-test.json', 'r') as f:\n",
        "  testing_set = json.load(f)[:400]\n",
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "class Model(nn.Module):\n",
        "  # same as train \n",
        "  def __init__(self, BERT_MODEL):\n",
        "    super(Model, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, 3) # Number of output classes = 3\n",
        "\n",
        "  def forward(self, ids, mask, token_type_ids):\n",
        "    last_hidden_state, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)\n",
        "\n",
        "model = torch.load('/content/drive/MyDrive/sentihood/1.bin')\n"
      ],
      "metadata": {
        "id": "ib_-agFomdp6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for each_example in tqdm(testing_set, ncols=80):\n",
        "  id = each_example['id']\n",
        "  text = each_example['text'].strip()\n",
        "\n",
        "  each_example['model_pred'] = []\n",
        "\n",
        "  count_location = 1\n",
        "  for location in locations:\n",
        "    if location in text:\n",
        "      # If \"location\" is present in the text, then utilize the trained model\n",
        "      # to predict the aspects and their corresponding sentiment of the text.\n",
        "\n",
        "      text = text.replace(location, 'location - ' + str(count_location))\n",
        "      \n",
        "      for aspect in aspects:\n",
        "        auxiliary_sentence = f'location - {str(count_location)} - {aspect}'\n",
        "        combined_text = text + ' ' + auxiliary_sentence\n",
        "        \n",
        "        inputs = tokenizer.encode_plus(\n",
        "            combined_text,\n",
        "            add_special_tokens = True,\n",
        "            max_length = max_length,\n",
        "            pad_to_max_length = True\n",
        "        )\n",
        "        ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).unsqueeze(0)\n",
        "        mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).unsqueeze(0)\n",
        "        token_type_ids = torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        predicted = predicted.detach().cpu().numpy()\n",
        "\n",
        "         # If predicted sentiment is not None, then add it to the preds.jsonl.\n",
        "         \n",
        "        if predicted[0] != 2:\n",
        "          result = {\n",
        "              \"sentiment\": labels_to_sentiment_dict[predicted[0]],\n",
        "              \"aspect\": aspect,\n",
        "              \"target_entity\": location\n",
        "          }\n",
        "          each_example['model_pred'].append(result)\n",
        "      \n",
        "    count_location += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II2_FnzjlpdM",
        "outputId": "ae13c9e5-e195-4032-fd48-ced5c1b7b623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|                                                   | 0/400 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|███████████████████████████████████████| 400/400 [1:00:26<00:00,  9.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/pred.jsonl', mode='w', encoding='utf-8') as fp:\n",
        "  for each in testing_set:\n",
        "    json_record = json.dumps(each, ensure_ascii=False)\n",
        "    fp.write(json_record + '\\n')"
      ],
      "metadata": {
        "id": "HRPxQTgw1ImO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0Qj2T-u-mrmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}